{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKvYiJgnYExi"
   },
   "source": [
    "This notebook provides examples to go along with the [textbook](https://underactuated.csail.mit.edu/dp.html).  I recommend having both windows open, side-by-side!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4QOaw_zYLfI"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "from matplotlib import cm\n",
    "from pydrake.all import (\n",
    "    BatchEvalTimeDerivatives,\n",
    "    BatchEvalUniquePeriodicDiscreteUpdate,\n",
    "    DiagramBuilder,\n",
    "    DiscreteAlgebraicRiccatiEquation,\n",
    "    LeafSystem,\n",
    "    LinearSystem,\n",
    "    MeshcatVisualizer,\n",
    "    MultilayerPerceptron,\n",
    "    PerceptronActivationType,\n",
    "    RandomGenerator,\n",
    "    Rgba,\n",
    "    RigidTransform,\n",
    "    RotationMatrix,\n",
    "    SceneGraph,\n",
    "    Simulator,\n",
    "    StartMeshcat,\n",
    "    ZeroOrderHold,\n",
    ")\n",
    "from pydrake.examples import AcrobotPlant, PendulumGeometry, PendulumPlant\n",
    "\n",
    "from underactuated.jupyter import running_as_notebook\n",
    "from underactuated.optimizers import Adam\n",
    "\n",
    "if running_as_notebook:\n",
    "    mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the visualizer (run this cell only once, each instance consumes a port)\n",
    "meshcat = StartMeshcat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Fitted Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the double integrator\n",
    "A = np.array([[0.0, 1.0], [0.0, 0.0]])\n",
    "B = np.array([[0.0], [1.0]])\n",
    "Q = 0.1 * np.eye(2)\n",
    "R = np.eye(1)\n",
    "\n",
    "\n",
    "# vectorized\n",
    "def min_time_cost(x, u):\n",
    "    return 1.0 - np.isclose(x, np.zeros((2, 1))).all(axis=0)\n",
    "\n",
    "\n",
    "def quadratic_regulator_cost(x, u):\n",
    "    return (x * (Q @ x)).sum(axis=0) + (u * (R @ u)).sum(axis=0)\n",
    "\n",
    "\n",
    "def min_time_solution(x, time_step, discount_factor=1):\n",
    "    # Caveat: this does not take the zero-order hold on u into account\n",
    "\n",
    "    q = x[0, :]\n",
    "    qdot = x[1, :]\n",
    "    # mask indicates that we are in the regime where u = +1.\n",
    "    mask = ((qdot < 0) & (2 * q <= (qdot**2))) | ((qdot >= 0) & (2 * q < -(qdot**2)))\n",
    "    T = np.empty(q.size)\n",
    "    T[mask] = 2 * np.sqrt(0.5 * qdot[mask] ** 2 - q[mask]) - qdot[mask]\n",
    "    T[~mask] = qdot[~mask] + 2 * np.sqrt(0.5 * qdot[~mask] ** 2 + q[~mask])\n",
    "\n",
    "    if discount_factor == 1:\n",
    "        return T\n",
    "    else:\n",
    "        # discount in continuous time looks like e^(-t/tau), with e^(-time_step/tau) = discount_factor; or -time_step/tau = ln(discount_factor)\n",
    "        tau = -time_step / np.log(discount_factor)\n",
    "        # ∫₀ᵀ exp(−t/τ) dt = τ [1 − exp(-T/τ)]\n",
    "        return tau * (1 - np.exp(-T / tau))\n",
    "\n",
    "\n",
    "def quadratic_regulator_solution(x, time_step, discount_factor=1):\n",
    "    S = DiscreteAlgebraicRiccatiEquation(\n",
    "        A=np.sqrt(discount_factor) * (np.eye(2) + time_step * A),\n",
    "        B=time_step * B,\n",
    "        Q=time_step * Q,\n",
    "        R=time_step * R / discount_factor,\n",
    "    )\n",
    "    return (x * (S @ x)).sum(axis=0)\n",
    "\n",
    "\n",
    "def plot_and_compare(mlp, context, running_cost, time_step, discount_factor=1.0):\n",
    "    x1s = np.linspace(-5, 5, 31)\n",
    "    x2s = np.linspace(-4, 4, 51)\n",
    "    X1s, X2s = np.meshgrid(x1s, x2s)\n",
    "    N = X1s.size\n",
    "    X = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "    J = np.zeros((1, N))\n",
    "\n",
    "    mlp.BatchOutput(context, X, J)\n",
    "\n",
    "    meshcat.PlotSurface(\n",
    "        \"Jhat\",\n",
    "        X1s,\n",
    "        X2s,\n",
    "        J.reshape(X1s.shape),\n",
    "        rgba=Rgba(0, 0, 1),\n",
    "        wireframe=True,\n",
    "    )\n",
    "\n",
    "    if running_cost == min_time_cost:\n",
    "        Jd = min_time_solution(X, time_step, discount_factor)\n",
    "    elif running_cost == quadratic_regulator_cost:\n",
    "        Jd = quadratic_regulator_solution(X, time_step, discount_factor)\n",
    "\n",
    "    meshcat.PlotSurface(\n",
    "        \"J_desired\",\n",
    "        X1s,\n",
    "        X2s,\n",
    "        Jd.reshape(X1s.shape),\n",
    "        rgba=Rgba(1, 0, 0),\n",
    "        wireframe=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's simply evaluate how well the network can fit the known cost-to-go functions (using supervised learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SupervisedDemo(running_cost, time_step, discount_factor=1.0):\n",
    "    x1s = np.linspace(-5, 5, 51)\n",
    "    x2s = np.linspace(-4, 4, 51)\n",
    "    X1s, X2s = np.meshgrid(x1s, x2s)\n",
    "    N = X1s.size\n",
    "    X = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "\n",
    "    if running_cost == min_time_cost:\n",
    "        Jd = min_time_solution(X, time_step, discount_factor)\n",
    "    elif running_cost == quadratic_regulator_cost:\n",
    "        Jd = quadratic_regulator_solution(X, time_step, discount_factor)\n",
    "\n",
    "    Jd = Jd.reshape((1, N))\n",
    "\n",
    "    mlp = MultilayerPerceptron(\n",
    "        [2, 64, 64, 1] if running_cost == min_time_cost else [2, 16, 16, 1],\n",
    "        [\n",
    "            PerceptronActivationType.kReLU,\n",
    "            PerceptronActivationType.kReLU,\n",
    "            PerceptronActivationType.kIdentity,\n",
    "        ],\n",
    "    )\n",
    "    context = mlp.CreateDefaultContext()\n",
    "    generator = RandomGenerator(152)\n",
    "    mlp.SetRandomContext(context, generator)\n",
    "\n",
    "    optimizer = Adam(mlp.GetMutableParameters(context))\n",
    "\n",
    "    dloss_dparams = np.zeros(mlp.num_parameters())\n",
    "    last_loss = np.inf\n",
    "    for epoch in range(2000 if running_as_notebook else 2):\n",
    "        loss = mlp.BackpropagationMeanSquaredError(context, X, Jd, dloss_dparams)\n",
    "        if epoch % 20 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"loss = {loss}\")\n",
    "        if np.linalg.norm(last_loss - loss) < 1e-6:\n",
    "            break\n",
    "        last_loss = loss\n",
    "        optimizer.step(loss, dloss_dparams)\n",
    "\n",
    "    plot_and_compare(mlp, context, running_cost, time_step, discount_factor)\n",
    "\n",
    "\n",
    "meshcat.Delete()\n",
    "SupervisedDemo(min_time_cost, 0.1, 0.98)\n",
    "# SupervisedDemo(quadratic_regulator_cost, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete time, continuous state, discrete action\n",
    "\n",
    "This is the standard \"fitted value iteration\" algorithm with a multilayer perceptron (MLP) as the function approximator, and a single step of gradient descent performed on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FittedValueIteration(running_cost, time_step, discount_factor=0.9):\n",
    "    x1s = np.linspace(-5, 5, 31)\n",
    "    x2s = np.linspace(-4, 4, 31)\n",
    "    us = np.linspace(-1, 1, 9)\n",
    "    Us, X1s, X2s = np.meshgrid(us, x1s, x2s, indexing=\"ij\")\n",
    "    XwithU = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "    UwithX = Us.flatten().reshape(1, -1)\n",
    "    Nx = x1s.size * x2s.size\n",
    "    X = XwithU[:, :Nx]\n",
    "    N = X1s.size\n",
    "\n",
    "    # TODO(russt): Use batch eval dynamics\n",
    "    system = LinearSystem(\n",
    "        np.eye(2) + time_step * A, time_step * B, np.eye(2), np.zeros((2, 1)), time_step\n",
    "    )\n",
    "    context = system.CreateDefaultContext()\n",
    "    # Xnext = XwithU + time_step * (A @ XwithU + B @ UwithX)\n",
    "    Xnext = BatchEvalUniquePeriodicDiscreteUpdate(\n",
    "        system, context, times=[0] * N, states=XwithU, inputs=UwithX\n",
    "    )\n",
    "    Cost = time_step * running_cost(XwithU, UwithX)\n",
    "    Jnext = np.zeros((1, N))\n",
    "    Jd = np.zeros((1, Nx))\n",
    "\n",
    "    mlp = MultilayerPerceptron(\n",
    "        [2, 100, 100, 1] if running_cost == min_time_cost else [2, 16, 16, 1],\n",
    "        [\n",
    "            PerceptronActivationType.kReLU,\n",
    "            PerceptronActivationType.kReLU,\n",
    "            PerceptronActivationType.kIdentity,\n",
    "        ],\n",
    "    )\n",
    "    context = mlp.CreateDefaultContext()\n",
    "    generator = RandomGenerator(123)\n",
    "    mlp.SetRandomContext(context, generator)\n",
    "\n",
    "    optimizer = Adam(mlp.GetMutableParameters(context))\n",
    "\n",
    "    plot_and_compare(mlp, context, running_cost, time_step, discount_factor)\n",
    "    dloss_dparams = np.zeros(mlp.num_parameters())\n",
    "    last_loss = np.inf\n",
    "    for epoch in range(500 if running_as_notebook else 2):\n",
    "        mlp.BatchOutput(context, Xnext, Jnext)\n",
    "        Jd[:] = np.min((Cost + discount_factor * Jnext).reshape(us.size, Nx), axis=0)\n",
    "        for i in range(100 if running_as_notebook else 2):\n",
    "            loss = mlp.BackpropagationMeanSquaredError(context, X, Jd, dloss_dparams)\n",
    "            optimizer.step(loss, dloss_dparams)\n",
    "        if np.linalg.norm(last_loss - loss) < 1e-8:\n",
    "            break\n",
    "        last_loss = loss\n",
    "        clear_output(wait=True)\n",
    "        print(f\"epoch {epoch}: loss = {loss}\")\n",
    "        if epoch % 10 == 0:\n",
    "            plot_and_compare(mlp, context, running_cost, time_step, discount_factor)\n",
    "\n",
    "    plot_and_compare(mlp, context, running_cost, time_step, discount_factor)\n",
    "\n",
    "\n",
    "# FittedValueIteration(min_time_cost, 0.1, discount_factor=0.95)\n",
    "\n",
    "FittedValueIteration(quadratic_regulator_cost, 0.1, discount_factor=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous-time, state, and actions\n",
    "\n",
    "I've written this to take an arbitrary system as the input.  It requires that the system has only continuous-time dynamics, and it assumes (currently without checking) that the system is control affine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ContinuousFittedValueIteration(\n",
    "    plant,\n",
    "    plant_context,\n",
    "    value_mlp,\n",
    "    state_cost_function,\n",
    "    R_diag,\n",
    "    state_samples,\n",
    "    time_step=0.01,\n",
    "    discount_factor=1.0,\n",
    "    input_port_index=0,\n",
    "    lr=0.001,\n",
    "    minibatch=None,\n",
    "    epochs=1000,\n",
    "    optim_steps_per_epoch=25,\n",
    "    input_limits=None,\n",
    "):\n",
    "    input_port = plant.get_input_port(input_port_index)\n",
    "    num_states = plant.num_continuous_states()\n",
    "    num_inputs = input_port.size()\n",
    "    N = state_samples.shape[1]\n",
    "\n",
    "    # assert plant.ValidateContext(plant_context)  # TODO(russt): bind this\n",
    "    assert plant_context.has_only_continuous_state()\n",
    "    assert value_mlp.get_input_port().size() == num_states\n",
    "    assert value_mlp.layers()[-1] == 1\n",
    "    assert R_diag.shape == (num_inputs,)\n",
    "    assert state_samples.shape[0] == num_states\n",
    "    assert time_step > 0.0\n",
    "    assert discount_factor > 0.0 and discount_factor <= 1.0\n",
    "    if input_limits != None:\n",
    "        assert (\n",
    "            num_inputs == 1\n",
    "        ), \"Input limits are only supported for scalar inputs (for now)\"\n",
    "        assert len(input_limits) == 2\n",
    "\n",
    "    mlp_context = value_mlp.CreateDefaultContext()\n",
    "    generator = RandomGenerator(123)\n",
    "    value_mlp.SetRandomContext(mlp_context, generator)\n",
    "\n",
    "    state_cost = state_cost_function(state_samples)\n",
    "    Rinv = 1 / R_diag\n",
    "\n",
    "    state = plant_context.get_mutable_continuous_state_vector()\n",
    "\n",
    "    # Precompute dynamics\n",
    "    u = np.zeros((num_inputs, N))\n",
    "    state_dynamics_x = BatchEvalTimeDerivatives(\n",
    "        plant, plant_context, times=[0] * N, states=state_samples, inputs=u\n",
    "    ).T\n",
    "    dstate_dynamics_du = [np.empty((num_states, N))] * num_inputs\n",
    "    for j in range(num_inputs):\n",
    "        u[j] = 1\n",
    "        dstate_dynamics_du[j] = BatchEvalTimeDerivatives(\n",
    "            plant, plant_context, times=[0] * N, states=state_samples, inputs=u\n",
    "        )\n",
    "        u[j] = 0\n",
    "\n",
    "    optimizer = Adam(value_mlp.GetMutableParameters(mlp_context), lr=lr)\n",
    "\n",
    "    M = minibatch if minibatch else N\n",
    "    J = np.zeros((1, M))\n",
    "    Jnext = np.zeros((1, M))\n",
    "    Jd = np.zeros((1, M))\n",
    "    dJdX = np.asfortranarray(np.zeros((num_states, M)))\n",
    "    dloss_dparams = np.zeros(value_mlp.num_parameters())\n",
    "    last_loss = np.inf\n",
    "    for epoch in range(epochs if running_as_notebook else 2):\n",
    "        if minibatch:\n",
    "            batch = np.random.randint(0, N, minibatch)\n",
    "        else:\n",
    "            batch = range(N)\n",
    "        value_mlp.BatchOutput(mlp_context, state_samples[:, batch], J, dJdX)\n",
    "        Xnext = state_samples[:, batch] + time_step * state_dynamics_x[batch, :].T\n",
    "        G = state_cost[batch]\n",
    "        for i in range(num_inputs):\n",
    "            ui = -0.5 * Rinv[i] * np.sum(dstate_dynamics_du[i][:, batch] * dJdX, 0)\n",
    "            if input_limits != None:\n",
    "                ui = np.minimum(np.maximum(ui, input_limits[0]), input_limits[1])\n",
    "            G += R_diag[i] * ui**2\n",
    "            Xnext += time_step * dstate_dynamics_du[i][:, batch] * ui\n",
    "        value_mlp.BatchOutput(mlp_context, Xnext, Jnext)\n",
    "        Jd[:] = G * time_step + discount_factor * Jnext\n",
    "        for i in range(optim_steps_per_epoch if running_as_notebook else 2):\n",
    "            loss = value_mlp.BackpropagationMeanSquaredError(\n",
    "                mlp_context, state_samples[:, batch], Jd, dloss_dparams\n",
    "            )\n",
    "            optimizer.step(loss, dloss_dparams)\n",
    "        if not minibatch and np.linalg.norm(last_loss - loss) < 1e-8:\n",
    "            break\n",
    "        last_loss = loss\n",
    "        if epoch % 20 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"epoch {epoch}: loss = {loss}\")\n",
    "\n",
    "    return mlp_context\n",
    "\n",
    "\n",
    "class ContinuousFittedValueIterationPolicy(LeafSystem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        plant,\n",
    "        value_mlp,\n",
    "        value_mlp_context,\n",
    "        R_diag,\n",
    "        input_port_index=0,\n",
    "        input_limits=None,\n",
    "    ):\n",
    "        LeafSystem.__init__(self)\n",
    "\n",
    "        num_plant_states = value_mlp.get_input_port().size()\n",
    "        self._plant = plant\n",
    "        self._plant_context = plant.CreateDefaultContext()\n",
    "\n",
    "        self.value_mlp = value_mlp\n",
    "        self.value_mlp_context = value_mlp_context\n",
    "        self.J = np.zeros((1, 1))\n",
    "        self.dJdX = np.asfortranarray(np.zeros((num_plant_states, 1)))\n",
    "\n",
    "        self.Rinv = 1 / R_diag\n",
    "        self.input_limits = input_limits\n",
    "        self.DeclareVectorInputPort(\"plant_state\", num_plant_states)\n",
    "        self._plant_input_port = self._plant.get_input_port(input_port_index)\n",
    "        self.DeclareVectorOutputPort(\n",
    "            \"output\", self._plant_input_port.size(), self.CalcOutput\n",
    "        )\n",
    "\n",
    "    def CalcOutput(self, context, output):\n",
    "        num_inputs = self._plant_input_port.size()\n",
    "        u = np.zeros(num_inputs)\n",
    "        plant_state = self.get_input_port().Eval(context)\n",
    "\n",
    "        self.value_mlp.BatchOutput(\n",
    "            self.value_mlp_context,\n",
    "            np.atleast_2d(plant_state).T,\n",
    "            self.J,\n",
    "            self.dJdX,\n",
    "        )\n",
    "\n",
    "        self._plant_context.SetContinuousState(plant_state)\n",
    "        self._plant_input_port.FixValue(self._plant_context, u)\n",
    "        state_dynamics_x = self._plant.EvalTimeDerivatives(\n",
    "            self._plant_context\n",
    "        ).CopyToVector()\n",
    "        for i in range(num_inputs):\n",
    "            u[i] = 1\n",
    "            self._plant_input_port.FixValue(self._plant_context, u)\n",
    "            dstate_dynamics_dui = (\n",
    "                self._plant.EvalTimeDerivatives(self._plant_context).CopyToVector()\n",
    "                - state_dynamics_x\n",
    "            )\n",
    "            ui = -0.5 * self.Rinv[i] * dstate_dynamics_dui.dot(self.dJdX)\n",
    "            if self.input_limits != None:\n",
    "                ui = np.minimum(\n",
    "                    np.maximum(ui, self.input_limits[0]), self.input_limits[1]\n",
    "                )\n",
    "            output.SetAtIndex(i, ui)\n",
    "            u[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0.0, 1.0], [0.0, 0.0]])\n",
    "B = np.array([[0.0], [1.0]])\n",
    "plant = LinearSystem(A, B, np.empty((0, 2)), np.empty((0, 1)))\n",
    "plant_context = plant.CreateDefaultContext()\n",
    "\n",
    "Q = np.eye(2)\n",
    "\n",
    "\n",
    "def quadratic_regulator_state_cost(x):\n",
    "    return (x * (Q @ x)).sum(axis=0)\n",
    "\n",
    "\n",
    "R_diag = np.array([1])\n",
    "R = np.eye(1)\n",
    "time_step = 0.01\n",
    "discount_factor = 0.9\n",
    "\n",
    "value_mlp = MultilayerPerceptron(\n",
    "    [2, 16, 16, 1],\n",
    "    [\n",
    "        PerceptronActivationType.kReLU,\n",
    "        PerceptronActivationType.kReLU,\n",
    "        PerceptronActivationType.kIdentity,\n",
    "    ],\n",
    ")\n",
    "\n",
    "x1s = np.linspace(-5, 5, 31)\n",
    "x2s = np.linspace(-4, 4, 31)\n",
    "X1s, X2s = np.meshgrid(x1s, x2s, indexing=\"ij\")\n",
    "state_samples = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "print(state_samples.shape)\n",
    "print(plant_context.num_continuous_states())\n",
    "value_mlp_context = ContinuousFittedValueIteration(\n",
    "    plant,\n",
    "    plant_context,\n",
    "    value_mlp,\n",
    "    quadratic_regulator_state_cost,\n",
    "    R_diag,\n",
    "    state_samples,\n",
    "    time_step=time_step,\n",
    "    discount_factor=discount_factor,\n",
    ")\n",
    "\n",
    "meshcat.Delete()\n",
    "meshcat.ResetRenderMode()\n",
    "plot_and_compare(\n",
    "    value_mlp,\n",
    "    value_mlp_context,\n",
    "    quadratic_regulator_cost,\n",
    "    time_step,\n",
    "    discount_factor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plant = PendulumPlant()\n",
    "plant_context = plant.CreateDefaultContext()\n",
    "\n",
    "Q = np.diag([10, 1])\n",
    "\n",
    "\n",
    "def quadratic_regulator_state_cost(x):\n",
    "    err = np.copy(x)\n",
    "    err[0] -= np.pi\n",
    "    return (err * (Q @ err)).sum(axis=0)\n",
    "\n",
    "\n",
    "R_diag = np.array([1])\n",
    "R = np.diag(R_diag)\n",
    "\n",
    "value_mlp = MultilayerPerceptron(\n",
    "    [True, False],\n",
    "    [100, 100, 1],\n",
    "    [\n",
    "        PerceptronActivationType.kReLU,\n",
    "        PerceptronActivationType.kReLU,\n",
    "        PerceptronActivationType.kIdentity,\n",
    "    ],\n",
    ")\n",
    "\n",
    "qs = np.linspace(0.0, 2.0 * np.pi, 51)\n",
    "qdots = np.linspace(-10.0, 10.0, 41)\n",
    "Qs, Qdots = np.meshgrid(qs, qdots)\n",
    "state_samples = np.vstack((Qs.flatten(), Qdots.flatten()))\n",
    "time_step = 0.01\n",
    "discount_factor = 0.999\n",
    "torque_limit = 3\n",
    "value_mlp_context = ContinuousFittedValueIteration(\n",
    "    plant,\n",
    "    plant_context,\n",
    "    value_mlp,\n",
    "    quadratic_regulator_state_cost,\n",
    "    R_diag,\n",
    "    state_samples,\n",
    "    time_step=time_step,\n",
    "    discount_factor=discount_factor,\n",
    "    minibatch=32,\n",
    "    lr=1e-5,\n",
    "    epochs=3000,\n",
    "    optim_steps_per_epoch=100,\n",
    "    input_limits=[-torque_limit, torque_limit],\n",
    ")\n",
    "\n",
    "J = value_mlp.BatchOutput(value_mlp_context, state_samples)\n",
    "fig = plt.figure(1, figsize=(9, 4))\n",
    "ax = fig.subplots()\n",
    "ax.set_xlabel(\"q\")\n",
    "ax.set_ylabel(\"qdot\")\n",
    "ax.set_title(\"Cost-to-Go\")\n",
    "ax.imshow(\n",
    "    J.reshape(qdots.size, qs.size),\n",
    "    cmap=cm.jet,\n",
    "    extent=(qs[0], qs[-1], qdots[-1], qdots[0]),\n",
    ")\n",
    "ax.invert_yaxis()\n",
    "ax.axis(\"auto\")\n",
    "display(plt.show());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(value_mlp, value_mlp_context, R_diag):\n",
    "    builder = DiagramBuilder()\n",
    "\n",
    "    scene_graph = builder.AddSystem(SceneGraph())\n",
    "    plant = builder.AddSystem(PendulumPlant())\n",
    "    PendulumGeometry.AddToBuilder(builder, plant.get_state_output_port(), scene_graph)\n",
    "\n",
    "    policy = builder.AddSystem(\n",
    "        ContinuousFittedValueIterationPolicy(\n",
    "            plant,\n",
    "            value_mlp,\n",
    "            value_mlp_context,\n",
    "            R_diag,\n",
    "            input_limits=[-torque_limit, torque_limit],\n",
    "        )\n",
    "    )\n",
    "    builder.Connect(plant.get_state_output_port(), policy.get_input_port())\n",
    "\n",
    "    zoh = builder.AddSystem(ZeroOrderHold(time_step, 1))\n",
    "    builder.Connect(policy.get_output_port(), zoh.get_input_port())\n",
    "    builder.Connect(zoh.get_output_port(), plant.get_input_port())\n",
    "\n",
    "    meshcat.Delete()\n",
    "    meshcat.Set2dRenderMode(\n",
    "        X_WC=RigidTransform(RotationMatrix.MakeZRotation(np.pi), [0, 1, 0])\n",
    "    )\n",
    "    vis = MeshcatVisualizer.AddToBuilder(builder, scene_graph, meshcat)\n",
    "\n",
    "    diagram = builder.Build()\n",
    "    simulator = Simulator(diagram)\n",
    "    context = simulator.get_mutable_context()\n",
    "    context.SetContinuousState([0.1, 0])\n",
    "    # simulator.set_target_realtime_rate(1.0 if running_as_notebook else 0.0)\n",
    "    vis.StartRecording(False)\n",
    "    simulator.AdvanceTo(4)\n",
    "    vis.PublishRecording()\n",
    "\n",
    "\n",
    "simulate(value_mlp, value_mlp_context, R_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acrobot\n",
    "\n",
    "Note: I haven't quite finished this example yet!  (coming soon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plant = AcrobotPlant()\n",
    "plant_context = plant.CreateDefaultContext()\n",
    "\n",
    "Q = np.diag([10, 10, 1, 1])\n",
    "\n",
    "\n",
    "def quadratic_regulator_state_cost(x):\n",
    "    err = np.copy(x)\n",
    "    err[0] -= np.pi\n",
    "    return (err * (Q @ err)).sum(axis=0)\n",
    "\n",
    "\n",
    "R_diag = np.array([1])\n",
    "R = np.diag(R_diag)\n",
    "\n",
    "value_mlp = MultilayerPerceptron(\n",
    "    [True, True, False, False],\n",
    "    [32, 32, 1],\n",
    "    [\n",
    "        PerceptronActivationType.kReLU,\n",
    "        PerceptronActivationType.kReLU,\n",
    "        PerceptronActivationType.kIdentity,\n",
    "    ],\n",
    ")\n",
    "\n",
    "q1s = np.linspace(0.0, 2.0 * np.pi, 21)\n",
    "q2s = np.linspace(0.0, 2.0 * np.pi, 21)\n",
    "q1dots = np.linspace(-10.0, 10.0, 11)\n",
    "q2dots = np.linspace(-10.0, 10.0, 11)\n",
    "Q1s, Q2s, Q1dots, Q2dots = np.meshgrid(q1s, q2s, q1dots, q2dots)\n",
    "state_samples = np.vstack(\n",
    "    (Q1s.flatten(), Q2s.flatten(), Q1dots.flatten(), Q2dots.flatten())\n",
    ")\n",
    "time_step = 0.01\n",
    "discount_factor = 0.95\n",
    "mlp_context = ContinuousFittedValueIteration(\n",
    "    plant,\n",
    "    plant_context,\n",
    "    value_mlp,\n",
    "    quadratic_regulator_state_cost,\n",
    "    R_diag,\n",
    "    state_samples,\n",
    "    time_step=time_step,\n",
    "    discount_factor=discount_factor,\n",
    "    lr=1e-5,\n",
    "    minibatch=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Underactuated Robotics - The Simple Pendulum.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
