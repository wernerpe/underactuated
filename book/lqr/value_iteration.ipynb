{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides examples to go along with the [textbook](https://underactuated.csail.mit.edu/lqr.html).  I recommend having both windows open, side-by-side!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from pydrake.all import DiscreteAlgebraicRiccatiEquation\n",
    "\n",
    "from underactuated import running_as_notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LQR via Fitted Value Iteration\n",
    "\n",
    "Double integrator.  Discrete-time, infinite-horizon, discounted.  This is the \"traditional\" fitted value iteration, $J=x^TSx$ as the (linear) function approximator.  It samples both $x$ and $u$, and takes an argmin over the samples $u$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the double integrator\n",
    "time_step = 0.1\n",
    "A = np.eye(2) + time_step * np.array([[0.0, 1.0], [0.0, 0.0]])\n",
    "B = time_step * np.array([[0.0], [1.0]])\n",
    "Q = time_step * np.eye(2)\n",
    "R = time_step * np.eye(1)\n",
    "\n",
    "\n",
    "def quadratic_regulator_cost(x, u):\n",
    "    return (x * (Q @ x)).sum(axis=0) + (u * (R @ u)).sum(axis=0)\n",
    "\n",
    "\n",
    "def FittedValueIteration(S_guess, gamma=0.9):\n",
    "    S_optimal = DiscreteAlgebraicRiccatiEquation(\n",
    "        A=np.sqrt(gamma) * A, B=B, Q=Q, R=R / gamma\n",
    "    )\n",
    "\n",
    "    x1s = np.linspace(-5, 5, 31)\n",
    "    x2s = np.linspace(-4, 4, 31)\n",
    "    us = np.linspace(-1, 1, 9)\n",
    "    Us, X1s, X2s = np.meshgrid(us, x1s, x2s, indexing=\"ij\")\n",
    "    XwithU = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "    UwithX = Us.flatten().reshape(1, -1)\n",
    "    Nx = x1s.size * x2s.size\n",
    "    X = XwithU[:, :Nx]\n",
    "    N = X1s.size\n",
    "\n",
    "    Xnext = A @ XwithU + B @ UwithX\n",
    "    Cost = quadratic_regulator_cost(XwithU, UwithX)\n",
    "    Jnext = np.zeros((1, N))\n",
    "    Jd = np.zeros((1, Nx))\n",
    "\n",
    "    def cost_to_go(S, X):\n",
    "        return (X * (S @ X)).sum(axis=0)  # vectorized quadratic form\n",
    "\n",
    "    def mean_bellman_residual(S, X, J_desired):\n",
    "        N = J_desired.size\n",
    "        err = cost_to_go(S, X) - J_desired\n",
    "        loss = np.mean(err**2)  # == 1/N ∑ᵢ [tr(Sxᵢxᵢ')-yᵢ]²\n",
    "        # dloss_dS = 2/N ∑ᵢ errᵢ*xᵢxᵢ' = 2/N X*Diag(err)*X'\n",
    "        dloss_dS = (\n",
    "            2\n",
    "            / N\n",
    "            * X\n",
    "            @ np.diag(\n",
    "                err.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            )\n",
    "            @ X.T\n",
    "        )\n",
    "        return loss, dloss_dS\n",
    "\n",
    "    S = S_guess\n",
    "    eta = 0.0001\n",
    "    last_loss = np.inf\n",
    "    for epoch in range(1000 if running_as_notebook else 2):\n",
    "        Jnext = cost_to_go(S, Xnext)\n",
    "        Jd[:] = np.min((Cost + gamma * Jnext).reshape(us.size, Nx), axis=0)\n",
    "        for i in range(10):\n",
    "            loss, dloss_dS = mean_bellman_residual(S, X, Jd)\n",
    "            S -= eta * dloss_dS\n",
    "        if epoch % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"epoch {epoch}: loss = {loss}, S = {S.flatten()}\")\n",
    "        if np.linalg.norm(last_loss - loss) < 1e-5:\n",
    "            break\n",
    "        last_loss = loss\n",
    "\n",
    "    print(f\"eigenvalues of S: {np.linalg.eig(S)[0]}\")\n",
    "    print(f\"optimal S= {S_optimal.flatten()}\")\n",
    "\n",
    "\n",
    "# FittedValueIteration(S_guess=np.eye(2), gamma=1.0)\n",
    "FittedValueIteration(S_guess=np.eye(2), gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the initial guess of $S$ and the discount factor $\\gamma$ that I've used above, the algorithm converges nicely.  You'll see that it is close to the optimal $S$, but not exactly the optimal $S$.\n",
    "\n",
    "Interestingly, if you choose $\\gamma$ to be much closer to 1, the algorithm will diverge. At some point, the sampled values over $u$ cause problems -- the Bellman equation does not actually have a steady-state solution for the discretized $u$ version of the problem (the cost-to-go is infinite).\n",
    "\n",
    "To convince you that this is indeed the problem, here is a version that solves analytically for the optimal $u$ (given the estimated $S$).  It draws samples only over $x$, and converges beautifully to the true optimum, even when $\\gamma=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FittedValueIteration(S_guess, gamma=0.9):\n",
    "    S_optimal = DiscreteAlgebraicRiccatiEquation(\n",
    "        A=np.sqrt(gamma) * A, B=B, Q=Q, R=R / gamma\n",
    "    )\n",
    "\n",
    "    x1s = np.linspace(-5, 5, 31)\n",
    "    x2s = np.linspace(-4, 4, 31)\n",
    "    X1s, X2s = np.meshgrid(x1s, x2s, indexing=\"ij\")\n",
    "    X = np.vstack((X1s.flatten(), X2s.flatten()))\n",
    "    N = X1s.size\n",
    "\n",
    "    Jd = np.zeros((1, N))\n",
    "\n",
    "    def cost_to_go(S, X):\n",
    "        return (X * (S @ X)).sum(axis=0)  # vectorized quadratic form\n",
    "\n",
    "    def policy(S, X):\n",
    "        return -gamma * np.linalg.inv(R + gamma * B.T @ S @ B) @ B.T @ S @ A @ X\n",
    "\n",
    "    def mean_bellman_residual(S, X, J_desired):\n",
    "        N = J_desired.size\n",
    "        err = cost_to_go(S, X) - J_desired\n",
    "        loss = np.mean(err**2)  # == 1/N ∑ᵢ [tr(Sxᵢxᵢ')-yᵢ]²\n",
    "        # dloss_dS = 2/N ∑ᵢ errᵢ*xᵢxᵢ' = 2/N X*Diag(err)*X'\n",
    "        dloss_dS = (\n",
    "            2\n",
    "            / N\n",
    "            * X\n",
    "            @ np.diag(\n",
    "                err.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            )\n",
    "            @ X.T\n",
    "        )\n",
    "        return loss, dloss_dS\n",
    "\n",
    "    S = S_guess\n",
    "    eta = 0.0001\n",
    "    last_loss = np.inf\n",
    "    for epoch in range(1000 if running_as_notebook else 2):\n",
    "        U = policy(S, X)\n",
    "        Xnext = A @ X + B @ U\n",
    "        G = quadratic_regulator_cost(X, U)\n",
    "        Jd = G + gamma * cost_to_go(S, Xnext)\n",
    "        for i in range(10):\n",
    "            loss, dloss_dS = mean_bellman_residual(S, X, Jd)\n",
    "            S -= eta * dloss_dS\n",
    "        if epoch % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"epoch {epoch}: loss = {loss}, S = {S.flatten()}\")\n",
    "        if np.linalg.norm(last_loss - loss) < 1e-5:\n",
    "            break\n",
    "        last_loss = loss\n",
    "\n",
    "    print(f\"eigenvalues of S: {np.linalg.eig(S)[0]}\")\n",
    "    print(f\"optimal S= {S_optimal.flatten()}\")\n",
    "    return S\n",
    "\n",
    "\n",
    "FittedValueIteration(S_guess=np.eye(2), gamma=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
